{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35af8cbb-8593-428b-bc6f-cac1d6e9ce9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "!conda install numpy scipy scikit-learn pandas joblib pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736551f-0126-4c68-b0b9-2a209d98921a",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install deap update_checker tqdm stopit xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a43741-6b6f-4a3f-98a2-bffe5bcd07ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285173c6-12d9-4781-8167-4e302ad48d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import timeit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b43423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\lostx\\Downloads\\Assignments\\Prepped_Data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5666e2b-1fef-4948-bc5c-b103e57002b4",
   "metadata": {},
   "source": [
    "importing and making sure my data is intact as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc97075-2488-435d-a04c-47129bd9fbd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7032 entries, 0 to 7031\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   tenure              7032 non-null   int64  \n",
      " 1   PhoneService        7032 non-null   int64  \n",
      " 2   Contract            7032 non-null   int64  \n",
      " 3   PaymentMethod       7032 non-null   int64  \n",
      " 4   MonthlyCharges      7032 non-null   float64\n",
      " 5   TotalCharges        7032 non-null   float64\n",
      " 6   Churn               7032 non-null   int64  \n",
      " 7   Total_Tenure_ratio  7032 non-null   float64\n",
      "dtypes: float64(3), int64(5)\n",
      "memory usage: 439.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc14d74c-f283-4940-832a-dfee259aedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = df.drop('Churn', axis=1)\n",
    "targets = df['Churn']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, stratify=targets, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a28d5-a0b7-46d7-8cb1-efad3718c5ca",
   "metadata": {},
   "source": [
    "I chose churn as my target for the dataset, as it is the variable we are trying to find correlation to, and then find ways to prevent for our corperate partners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba921e74-5c0b-4871-83ec-4ed9dc5173ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7961696808367131\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7961696808367131\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.7961696808367131\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.7974970547766577\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.7974970547766577\n",
      "\n",
      "Best pipeline: RandomForestClassifier(MinMaxScaler(input_matrix), bootstrap=True, criterion=entropy, max_features=0.2, min_samples_leaf=8, min_samples_split=19, n_estimators=100)\n",
      "0.7923777019340159\n",
      "CPU times: total: 41.5 s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, cv=5,random_state=42, scoring='accuracy', verbosity=2, n_jobs=-1)\n",
    "\n",
    "tpot.fit(x_train, y_train)\n",
    "print(tpot.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945ba4e-d8ae-4a97-86a4-0b2314b70e70",
   "metadata": {},
   "source": [
    "This took less time than I thought it would, but i am happy to see the autoML comparing the data for me as opposed to what we have done in previous weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "273d6cb1-34a0-4427-a784-2bd1750d779a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tpot.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac0a29-e180-40ec-9e1f-bcbfb664c40b",
   "metadata": {},
   "source": [
    "Below is my comparison between the Prediction and my actual for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff5eed1f-a13f-493b-89d8-2a1d7041231a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for test data set\n",
      "[1 0 0 ... 0 0 1]\n",
      "Actuals for test data set\n",
      "942     1\n",
      "503     0\n",
      "4848    1\n",
      "4143    0\n",
      "5875    1\n",
      "       ..\n",
      "3678    0\n",
      "6245    0\n",
      "5496    0\n",
      "3837    1\n",
      "5719    1\n",
      "Name: Churn, Length: 1758, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Predictions for test data set')\n",
    "print(predictions)\n",
    "print('Actuals for test data set')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d7ef260-38bd-4afc-bc3d-40e4c8553660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the TPOT predictions: 0.7923777019340159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Accuracy of the TPOT predictions: {accuracy_score(y_test,predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c17110",
   "metadata": {},
   "source": [
    "Next, i will save my TPOT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47095f",
   "metadata": {},
   "source": [
    "Next, we want to save our trained model so we can use it in a Python file later. Afterall, having a model that is only available on your local machine isn't idea in a corporate setting. You will want to by able to deploy your model into a production environment. \n",
    "\n",
    "Most production environments don't utilize Jupyter notebooks. The `export()` command will create a native python file for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2098bc67-7b4f-41cd-bb62-e9a67eefd605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpot.export('tpot_churn_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4314d795-0964-49a4-bc7b-861e4b5ee5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">RandomForestClassifier</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">make_pipeline</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">MinMaxScaler</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">tpot.export_utils</span> <span class=\"kn\">import</span> <span class=\"n\">set_param_recursive</span>\n",
       "\n",
       "<span class=\"c1\"># NOTE: Make sure that the outcome column is labeled &#39;target&#39; in the data file</span>\n",
       "<span class=\"n\">tpot_data</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;PATH/TO/DATA/FILE&#39;</span><span class=\"p\">,</span> <span class=\"n\">sep</span><span class=\"o\">=</span><span class=\"s1\">&#39;COLUMN_SEPARATOR&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float64</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">tpot_data</span><span class=\"o\">.</span><span class=\"n\">drop</span><span class=\"p\">(</span><span class=\"s1\">&#39;target&#39;</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">training_features</span><span class=\"p\">,</span> <span class=\"n\">testing_features</span><span class=\"p\">,</span> <span class=\"n\">training_target</span><span class=\"p\">,</span> <span class=\"n\">testing_target</span> <span class=\"o\">=</span> \\\n",
       "            <span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">tpot_data</span><span class=\"p\">[</span><span class=\"s1\">&#39;target&#39;</span><span class=\"p\">],</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Average CV score on the training set was: 0.7974970547766577</span>\n",
       "<span class=\"n\">exported_pipeline</span> <span class=\"o\">=</span> <span class=\"n\">make_pipeline</span><span class=\"p\">(</span>\n",
       "    <span class=\"n\">MinMaxScaler</span><span class=\"p\">(),</span>\n",
       "    <span class=\"n\">RandomForestClassifier</span><span class=\"p\">(</span><span class=\"n\">bootstrap</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"o\">=</span><span class=\"s2\">&quot;entropy&quot;</span><span class=\"p\">,</span> <span class=\"n\">max_features</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"n\">min_samples_leaf</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">min_samples_split</span><span class=\"o\">=</span><span class=\"mi\">19</span><span class=\"p\">,</span> <span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n",
       "<span class=\"p\">)</span>\n",
       "<span class=\"c1\"># Fix random state for all the steps in exported pipeline</span>\n",
       "<span class=\"n\">set_param_recursive</span><span class=\"p\">(</span><span class=\"n\">exported_pipeline</span><span class=\"o\">.</span><span class=\"n\">steps</span><span class=\"p\">,</span> <span class=\"s1\">&#39;random_state&#39;</span><span class=\"p\">,</span> <span class=\"mi\">42</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"n\">exported_pipeline</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">training_features</span><span class=\"p\">,</span> <span class=\"n\">training_target</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">exported_pipeline</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">testing_features</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{pandas} \\PY{k}{as} \\PY{n+nn}{pd}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{ensemble} \\PY{k+kn}{import} \\PY{n}{RandomForestClassifier}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{model\\PYZus{}selection} \\PY{k+kn}{import} \\PY{n}{train\\PYZus{}test\\PYZus{}split}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{pipeline} \\PY{k+kn}{import} \\PY{n}{make\\PYZus{}pipeline}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{sklearn}\\PY{n+nn}{.}\\PY{n+nn}{preprocessing} \\PY{k+kn}{import} \\PY{n}{MinMaxScaler}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{tpot}\\PY{n+nn}{.}\\PY{n+nn}{export\\PYZus{}utils} \\PY{k+kn}{import} \\PY{n}{set\\PYZus{}param\\PYZus{}recursive}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} NOTE: Make sure that the outcome column is labeled \\PYZsq{}target\\PYZsq{} in the data file}\n",
       "\\PY{n}{tpot\\PYZus{}data} \\PY{o}{=} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{read\\PYZus{}csv}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PATH/TO/DATA/FILE}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{n}{sep}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{COLUMN\\PYZus{}SEPARATOR}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{np}\\PY{o}{.}\\PY{n}{float64}\\PY{p}{)}\n",
       "\\PY{n}{features} \\PY{o}{=} \\PY{n}{tpot\\PYZus{}data}\\PY{o}{.}\\PY{n}{drop}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{target}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{n}{axis}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)}\n",
       "\\PY{n}{training\\PYZus{}features}\\PY{p}{,} \\PY{n}{testing\\PYZus{}features}\\PY{p}{,} \\PY{n}{training\\PYZus{}target}\\PY{p}{,} \\PY{n}{testing\\PYZus{}target} \\PY{o}{=} \\PYZbs{}\n",
       "            \\PY{n}{train\\PYZus{}test\\PYZus{}split}\\PY{p}{(}\\PY{n}{features}\\PY{p}{,} \\PY{n}{tpot\\PYZus{}data}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{target}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{n}{random\\PYZus{}state}\\PY{o}{=}\\PY{l+m+mi}{42}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Average CV score on the training set was: 0.7974970547766577}\n",
       "\\PY{n}{exported\\PYZus{}pipeline} \\PY{o}{=} \\PY{n}{make\\PYZus{}pipeline}\\PY{p}{(}\n",
       "    \\PY{n}{MinMaxScaler}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "    \\PY{n}{RandomForestClassifier}\\PY{p}{(}\\PY{n}{bootstrap}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{,} \\PY{n}{criterion}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{entropy}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{n}{max\\PYZus{}features}\\PY{o}{=}\\PY{l+m+mf}{0.2}\\PY{p}{,} \\PY{n}{min\\PYZus{}samples\\PYZus{}leaf}\\PY{o}{=}\\PY{l+m+mi}{8}\\PY{p}{,} \\PY{n}{min\\PYZus{}samples\\PYZus{}split}\\PY{o}{=}\\PY{l+m+mi}{19}\\PY{p}{,} \\PY{n}{n\\PYZus{}estimators}\\PY{o}{=}\\PY{l+m+mi}{100}\\PY{p}{)}\n",
       "\\PY{p}{)}\n",
       "\\PY{c+c1}{\\PYZsh{} Fix random state for all the steps in exported pipeline}\n",
       "\\PY{n}{set\\PYZus{}param\\PYZus{}recursive}\\PY{p}{(}\\PY{n}{exported\\PYZus{}pipeline}\\PY{o}{.}\\PY{n}{steps}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{random\\PYZus{}state}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+m+mi}{42}\\PY{p}{)}\n",
       "\n",
       "\\PY{n}{exported\\PYZus{}pipeline}\\PY{o}{.}\\PY{n}{fit}\\PY{p}{(}\\PY{n}{training\\PYZus{}features}\\PY{p}{,} \\PY{n}{training\\PYZus{}target}\\PY{p}{)}\n",
       "\\PY{n}{results} \\PY{o}{=} \\PY{n}{exported\\PYZus{}pipeline}\\PY{o}{.}\\PY{n}{predict}\\PY{p}{(}\\PY{n}{testing\\PYZus{}features}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.pipeline import make_pipeline\n",
       "from sklearn.preprocessing import MinMaxScaler\n",
       "from tpot.export_utils import set_param_recursive\n",
       "\n",
       "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
       "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
       "features = tpot_data.drop('target', axis=1)\n",
       "training_features, testing_features, training_target, testing_target = \\\n",
       "            train_test_split(features, tpot_data['target'], random_state=42)\n",
       "\n",
       "# Average CV score on the training set was: 0.7974970547766577\n",
       "exported_pipeline = make_pipeline(\n",
       "    MinMaxScaler(),\n",
       "    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.2, min_samples_leaf=8, min_samples_split=19, n_estimators=100)\n",
       ")\n",
       "# Fix random state for all the steps in exported pipeline\n",
       "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
       "\n",
       "exported_pipeline.fit(training_features, training_target)\n",
       "results = exported_pipeline.predict(testing_features)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Code\n",
    "\n",
    "Code('tpot_churn_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff7ff9",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66351b6d",
   "metadata": {},
   "source": [
    "I installed tpot to use as my AutoML for my churn dataset, and then ran multiple generations testing for the best accuracy in the set of tests, which was 0.7923777019340159. I'm not sure if this was any more accurate than when the action was performed by hand, but it took less time, which is great. I then exported my .py file and uploaded it to the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da3195-763f-4a5e-a977-b2841be71e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
